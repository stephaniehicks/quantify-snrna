---
title: "download data from GEO"
author: "Albert Kuo"
date: "6/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Packages
library(here)
library(tidyverse)
```

# Download data

Run the shell script `download-geo-data.sh`. 

Information about the paper whose data we are downloading:

* https://www.nature.com/articles/s41587-020-0465-8
* Systematic comparison of single-cell and single-nucleus RNA-sequencing methods
* Ding	2020
* Mouse cortex	~7000?
* UMIs and reads
* RNA-seq data generated in this project are available from the Gene Expression Omnibus with accession number **GSE132044** and the Single Cell Portal with accession numbers SCP424, SCP425 and SCP426. https://singlecell.broadinstitute.org/single_cell/study/SCP425/single-cell-comparison-cortex-data#study-summary

In the paper there were the following types of data: 

1. human PBMC, scRNA-seq
2. mouse cortex, scRNA-seq (processed on 4 platforms)
3. human cell line and PBMC, bulk RNA-seq
4. mouse cell line and cortex, bulk RNA-seq
5. mixture of human and murine cell line, scRNA-seq

We are going to focus on the mouse cortex. 

```{r, eval=FALSE}
gse <- GEOquery::getGEO("GSE132044")
sapply(gse, dim) # Number of rows per entry
gse <- gse[[2]] # this contains all the mouse cortex data on 4 platforms
pdata <- Biobase::pData(gse) # Get phenotype table
pdata$Experiment <- sapply(stringr::str_split(pdata$relation.1, "="), tail, 1)
```

### Download raw SRA files 

#### To download phenotype data from GEO and join the `SRR` IDs

Go here: https://www.ncbi.nlm.nih.gov/bioproject/PRJNA545730. 
Then click on `SRA Experiments`. Click `Send to`. Choose `File`. 
Change Format to `RunInfo`. Click `Create File`. 

This will download a file called `SraRunInfo.csv`. Read this file in. 

```{r, eval=FALSE}
sra <- readr::read_csv(here("mouse_cortex", "files", "SraRunInfo.csv"))
pdata <- dplyr::left_join(pdata, sra, by = "Experiment")

# save phenotype table
readr::write_csv(pdata, path = here("mouse_cortex", "files", "pdata.csv"))
```

#### Create subsets of the table

```{r}
pdata <- readr::read_csv(file = here("mouse_cortex", "files", "pdata.csv"))

pdata %>%
  select(Experiment, title, organism_ch1, source_name_ch1, description, 
         description.1, starts_with("characteristics")) %>% 
  head(n=6)

# shows there are four types of platforms used for library construction
table(pdata$characteristics_ch1.3)
```

###  10x Chromium (v2) platform

Let's start with the 10X platform. 
```{r}
sra_meta <- pdata %>% 
  select(Run, Experiment, title, geo_accession, type, source_name_ch1, organism_ch1, starts_with("characteristics"), molecule_ch1, taxid_ch1, description, description.1,
         platform_id, instrument_model, library_selection, library_source, library_strategy,
         `library construction:ch1`, `age:ch1`, `Sex:ch1`, `strain:ch1`, spots, bases, spots_with_mates, avgLength, size_MB, AssemblyName, SRAStudy, BioProject)
write_csv(sra_meta, path = here("mouse_cortex", "files", "SRA_geo_metadata.csv"))

# 16 samples (10x)
pdata_10x <- pdata[pdata$characteristics_ch1.3 == 
                     "library construction: 10x Chromium (v2)",] 

write.table(pdata_10x$Run, file = here("mouse_cortex", "files", "SRR_files_10x.txt"), 
            quote= FALSE,row.names = FALSE, col.names = FALSE)
write.table(pdata_10x$download_path, file = here("mouse_cortex", "files", "SRR_paths_10x.txt"), 
            quote= FALSE,row.names = FALSE, col.names = FALSE)

# Save separate files for cortex1 and cortex2
cortex1_runs = pdata_10x %>%
  separate(title, c("cortex", "flow_cell", "seq", "lane")) %>%
  filter(cortex == "Cortex1") %>%
  pull(Run)
write.table(cortex1_runs, file = here("mouse_cortex", "files", "SRR_files_10x_cortex1.txt"), 
            quote= FALSE,row.names = FALSE, col.names = FALSE)

cortex2_runs = pdata_10x %>%
  separate(title, c("cortex", "flow_cell", "seq", "lane")) %>%
  filter(cortex == "Cortex2") %>%
  pull(Run)
write.table(cortex2_runs, file = here("mouse_cortex", "files", "SRR_files_10x_cortex2.txt"), 
            quote= FALSE,row.names = FALSE, col.names = FALSE)
```

### DroNc-seq

Will fill in later. 

### sci-RNA-seq 

Will fill in later. 

### Smart-seq2

Will fill in later. 

## Download SRA files 

### To install the SRA tool kit

Two relevant pages to install SRA toolkit

1. https://www.ncbi.nlm.nih.gov/sra/docs/sradownload/
2. https://github.com/ncbi/sra-tools/wiki/05.-Toolkit-Configuration (old version is https://ncbi.github.io/sra-tools/install_config.html )

The second one is relevant on where you want the data downloaded as a default. 

```{bash, eval = FALSE}
cd src 
wget "http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-centos_linux64.tar.gz"
tar -xzf sratoolkit.current-centos_linux64.tar.gz
```

You will need to add the Toolkit functions to your PATH variable (https://github.com/ncbi/sra-tools/wiki/02.-Installing-SRA-Toolkit). This can be done by adding ~/.../sratoolkit.2.10.6-centos_linux64/bin to the PATH variable in ~/.bash_profile. Check that it worked with `which fastq-dump`.

### Download the `.sra` files

Following the 2nd link, I changed default location to download data to be at 
`/fastscratch/myscratch/shicks1`. 

You can run interactively: 

```{bash}
cd /fastscratch/myscratch/shicks1
prefetch SRR9169228
```

Or run the shell script `download-sra-data.sh`.

```{bash}
prefetch --option-file SRR_files_10x.txt
```

where `SRR_files_10x.txt` is defined above and the first few lines look like this: 

SRR9169228
SRR9169229
SRR9169230
SRR9169231

## Extract FASTQ files

Finally, the the `fasterq-dump` function converts the prefetched `.sra` files
from compressed SRA format to fastq. 
The `-e 6` argument controls the number of threads you request. 
Default is 6. 

References:
1. https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump
2. https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?view=toolkit_doc&f=fastq-dump

Run the shell script `extract-fastq.sh`.

However, if you want to try and run it interactively, use this: 

```{bash}
cd sra
fastq-dump --split-files --gzip SRR9169228
fasterq-dump --split-files --include-technical SRR9169228.sra
```

Pulling from the [Reference 1](https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump), 
the default output is 

> The spots are split into (biological) reads, for each read - 4 lines of FASTQ are written. For spots having 2 reads, the reads are written into the *_1.fastq and *_2.fastq files. Unmated reads are placed in *_3.fastq.

So I think the `*_3.fastq` contains the barcodes. 
The files we want to use are `*_1.fastq` and `*_2.fastq`. 
